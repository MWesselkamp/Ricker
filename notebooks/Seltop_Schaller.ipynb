{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing the forecast performance of a process-based and model-free forecasting model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Empirical Dynamical Modelling is a modeling technique that relies on Takens theorem. This says, that we can reconstruct a representation of the system dynamics from system state measurements, available as an univariate timeseries (https://sugiharalab.github.io/EDM_Documentation/edm_intro/). When we want to make forecasts about a system, this technique can be applied in cases where no process model is available.\n",
    "\n",
    "Are mechanistic models that condense theory of our systems still useful? How does the EDM approach perform in forecasting in comparison to a (good) process model? This is the question we will try to answer in a pure simulation framework. We use an EDM and a process model $M_{f}$ to forecast the dynamics that we itself generate with an observation model $M_{obs}$.\n",
    "\n",
    "The goodness of the forecast model will be varied through the parameter for the intrinsic growth rate, $r$ (= *lambda* in models.py). Use as forecast model $M_{f}$ models.Ricker_Single (without temperature effects) and as observation model $M_{obs}$ models.Ricker_Multi (see models.py).\n",
    "1. Setting: We assume we have good knowledge about $r$ in the process model. Use the same $r$ for the process model and the observation model and vary simultaneously.\n",
    "2. Setting: We assume stepwise decreasing knowledge about the true parameter $r$, by increasing difference in $r_{obs}$ and $r_{f}$. Fix $r_{f}$ and slowly increase $r_{obs}$.\n",
    "3. Compare the $M_{f}$ Ricker and the EDM in \"goodness\" (using for example $R^2$) across $r$ for both settings.\n",
    "4. Use a gradient of goodness (referring to a gradient in $r$) to assess the relationship between maximum and real forecast horizon.\n",
    "\n",
    "Final hypothesis:\n",
    "$\\Delta(h_{real},h_{max}) \\sim g(f(y), y) + WPE (y)$\n",
    "Where $g$ is some function of model goodness and WPE the weighted permutation entropy.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The forecast horizon\n",
    "\n",
    "The forecast horizon is the limit in time, after which the proficiency (some distance, accuracy or association measure $d$) of the ensemble forecast to forecast the observation on average falls below a forecast proficiency threshold $\\rho$.\n",
    "Formalized, the forecast horizon $h$ is defined as\n",
    "$h := min_{t}(E[d(y_{f,i}, y_{obs})] < \\rho)$.\n",
    "We compute the forecast horizon of the model twice: Once for the observations $y_{obs}$ and once for the model itself, the mean ensemble trajectory $overline{Y_{f}}$. While the latter we call the real forecast horizon, $h_{real}$, the former will be termed the maximum forecast horizon $h_{max}$\n",
    "$h_{max} := min_{t}(E[overline{Y_{f}}, y_{obs})] < \\rho)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The weighted permutation entropy\n",
    "\n",
    "The (weighted) permutation entropy can be taken as a measure of intrinsic predictability of a system, given measurements of its state variable. It's robust to observational and process noise even if the dynamic exhibits chaotic behaviour. In the study of Pennenkamp et al. 2019, the relationship between the forecast error and the WPE was found to be positive. As such, we can expect the WPE to have a negative effect also on the difference between the maximum and the real forecast horizon by affecting both, the realisable forecast horizon and the model goodness, quantified as $g$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code examples\n",
    "\n",
    "Load the required packages."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Ricker')\n",
    "\n",
    "import models\n",
    "from scipy.stats import pearsonr, ttest_ind, rankdata\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import factorial\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute the real and the maximum forecast horizon of the model.\n",
    "\n",
    "2. Simulate observations and an according forecast ensemble under the specific choices of parameters for both, the observation model and the forecast model.\n",
    "3. Compute the model goodness, here as $R^2$\n",
    "4. Compute the forecast horizon based on a proficiency metric and a forecast proficiency threshold (here: Pearson's r with a threshold of 0.5).\n",
    "5. Determine the difference $\\Delta$ in $h_{max}$ and $h_{min}$ with a t-test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "19.0035"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate():\n",
    "\n",
    "    init_s = 0.99\n",
    "    r = 0.05\n",
    "    r1= 0.05\n",
    "    r2= 0.05\n",
    "\n",
    "    mf_pars = {'lambda': r, 'K': 1}\n",
    "    mf_errs = {\"sigma\":0.0001,\"phi\":0.0, \"init_u\":1e-4}\n",
    "\n",
    "    mo_pars = {'lambda1': r1+0.0001, 'K1': 1, 'alpha':1, 'beta':0.00006, 'lambda2': r2, 'K2': 1, 'gamma':1, 'delta':0.00005}\n",
    "    mo_errs = {\"sigma\":0.0001,\"phi\":0.0002, \"init_u\":1e-4}\n",
    "\n",
    "    sf_pars = {\"iterations\":2*52, \"initial_size\": init_s, \"ensemble_size\": 25}\n",
    "    so_pars = {\"iterations\":2*52, \"initial_size\": (init_s, init_s), \"ensemble_size\": 1}\n",
    "\n",
    "    mf = models.Ricker_Single(set_seed=False)\n",
    "    mo = models.Ricker_Multi(set_seed=False)\n",
    "\n",
    "    mf.parameters(mf_pars, mf_errs)\n",
    "    mo.parameters(mo_pars, mo_errs)\n",
    "\n",
    "    yf = mf.simulate(sf_pars)['ts']\n",
    "    yo = mo.simulate(so_pars)['ts'][:,:,0].squeeze()\n",
    "\n",
    "    return yf, yo\n",
    "\n",
    "def proficiency(yf, yo):\n",
    "\n",
    "    window = 3\n",
    "\n",
    "    d = []\n",
    "    for j in range(len(yo) - window):\n",
    "        d.append(pearsonr(yo[j:j + window], yf[j:j + window])[0])\n",
    "\n",
    "    return np.array(d)\n",
    "\n",
    "def model_goodness(yf, yo):\n",
    "\n",
    "    r2 = []\n",
    "    for i in range(yf.shape[0]):\n",
    "        r2.append(r2_score(yo, yf[i, :]))\n",
    "    return np.array(r2)\n",
    "\n",
    "def horizon(yf, yo, rho = 0.5):\n",
    "\n",
    "    p = []\n",
    "    for i in range(yf.shape[0]):\n",
    "\n",
    "        p.append(proficiency(yf[i,:], yo))\n",
    "\n",
    "    p = np.array(p)\n",
    "    p_reached = (np.mean(p, axis=0) < rho)\n",
    "\n",
    "    if p_reached.sum() == 0:\n",
    "        fh = len(yo)\n",
    "    else:\n",
    "        fh = np.argmax(p_reached)\n",
    "\n",
    "    return fh\n",
    "\n",
    "fh_max = []\n",
    "fh_real = []\n",
    "for i in range(20):\n",
    "    yf, yo = simulate()\n",
    "    r2 = model_goodness(yf, yo)\n",
    "    fh_max.append(horizon(yf, yf.mean(axis=0)))\n",
    "    fh_real.append(horizon(yf, yo))\n",
    "\n",
    "np.round(ttest_ind(fh_max, fh_real)[0], 4)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example computation of the permutation entropy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6678522378330369"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on Pennekamp 2019\n",
    "# PE based on an embedded time series\n",
    "def embed(x, m, d = 1):\n",
    "    \"\"\"\n",
    "    Pennekamp 2019\n",
    "    \"\"\"\n",
    "    n = len(x) - (m-1)*d\n",
    "    X = np.arange(len(x))\n",
    "    out = np.array([X[np.arange(n)]]*m)\n",
    "    a = np.repeat(np.arange(1, m)*d, out.shape[1])\n",
    "    out[1:,] = out[1:,]+a.reshape(out[1:,].shape)\n",
    "    out = x[out]\n",
    "\n",
    "    return out\n",
    "\n",
    "def entropy(wd):\n",
    "    \"\"\"\n",
    "    in bits\n",
    "    \"\"\"\n",
    "    return -np.sum(list(wd.values())*np.log2(list(wd.values())))\n",
    "\n",
    "\n",
    "def word_distr(x_emb, tie_method='average'):\n",
    "\n",
    "    words = [np.array2string(rankdata(x_emb[:, i])) for i in range(x_emb.shape[1])]\n",
    "    c = dict(Counter(words))\n",
    "    for k, v in c.items():\n",
    "        c[k] = v/len(words)\n",
    "    return c\n",
    "\n",
    "def permutation_entropy(x, m, d=1):\n",
    "\n",
    "    x_emb = embed(x, m=m, d=d)\n",
    "    wd = word_distr(x_emb)\n",
    "    denom = np.log2(2 * factorial(m))\n",
    "    ent = entropy(wd) / denom\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "x = np.random.normal(0,1,30)\n",
    "permutation_entropy(x, m = 3)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}